{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFg-hej_WCcg"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "# Initialize the LLM\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "\n",
        "# Define the input text\n",
        "input_text = \"your_long_input_text\"\n",
        "\n",
        "# Determine the maximum number of tokens from documentation\n",
        "max_tokens = 4097\n",
        "\n",
        "# # Split the input text into chunks based on the max tokens\n",
        "# text_chunks = split_text_into_chunks(input_text, max_tokens)\n",
        "\n",
        "# # Process each chunk separately\n",
        "# results = []\n",
        "# for chunk in text_chunks:\n",
        "#     result = llm.process(chunk)\n",
        "#     results.append(result)\n",
        "\n",
        "# # Combine the results as needed\n",
        "# final_result = combine_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDl-ccNpcaiT",
        "outputId": "36d33bb7-8bfa-469e-c7bb-ce81474f34a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Rainbow Socks Co.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPzW3tdCf-Hv",
        "outputId": "c0819dba-c67f-4d83-9c39-3cfd661db967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens Used: 46\n",
            "\tPrompt Tokens: 4\n",
            "\tCompletion Tokens: 42\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0009199999999999999\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2)\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    result = llm(\"Tell me a joke\")\n",
        "    print(cb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gysntr1Cl_Zy"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the weather like?\",\n",
        "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
        "    }, {\n",
        "        \"query\": \"How old are you?\",\n",
        "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create an example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "assistant. The assistant is known for its humor and wit, providing\n",
        "entertaining and amusing responses to users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few-shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qSsdednel_Tc",
        "outputId": "dff3876d-0b51-40eb-f051-53f00d316071"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' To live life to the fullest and enjoy the journey!'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
        "chain.run(\"What's the meaning of life?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKMKS9VKv6sc"
      },
      "source": [
        "# ===="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjKmhppJoqFf"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(\n",
        "        template=template,\n",
        "    input_variables=['question']\n",
        ")\n",
        "\n",
        "# user question\n",
        "question = \"What is the capital city of France?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDjVQUXVn3to",
        "outputId": "05a58495-47fb-4478-a902-839f3a5b102b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "paris\n"
          ]
        }
      ],
      "source": [
        "from langchain import HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize Hub LLM\n",
        "hub_llm = HuggingFaceHub(\n",
        "        repo_id='google/flan-t5-large',\n",
        "    model_kwargs={'temperature':0}\n",
        ")\n",
        "\n",
        "# create prompt template > LLM chain\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=hub_llm\n",
        ")\n",
        "\n",
        "# ask the user question about the capital of France\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysRU0joOn67n"
      },
      "outputs": [],
      "source": [
        "qa = [\n",
        "    {'question': \"What is the capital city of France?\"},\n",
        "    {'question': \"What is the largest mammal on Earth?\"},\n",
        "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
        "    {'question': \"What color is a ripe banana?\"}\n",
        "]\n",
        "res = llm_chain.generate(qa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayCQKxokueeK",
        "outputId": "e542716e-c01e-4e1d-e53a-df22e4081773"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='paris', generation_info=None)], [Generation(text='giraffe', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='yellow', generation_info=None)]], llm_output=None)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "qMkvO8I4sNYV",
        "outputId": "e67dfa09-f01c-4608-c360-bc6716c64958"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"1. The capital city of France is Paris.\\n2. The largest mammal on Earth is the blue whale.\\n3. The gas that is most abundant in Earth's atmosphere is nitrogen.\\n4. A ripe banana is yellow.\""
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"What is the capital city of France?\\n\" +\n",
        "    \"What is the largest mammal on Earth?\\n\" +\n",
        "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
        "\t\t\"What color is a ripe banana?\\n\"\n",
        ")\n",
        "llm_chain.run(qs_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbH_xc9PsS10"
      },
      "source": [
        "# ===="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_upxEcTGsNOs"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUIET_5msUMC"
      },
      "outputs": [],
      "source": [
        "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
        "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
        "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "letoflbjsp8V"
      },
      "outputs": [],
      "source": [
        "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
        "summarized_text = summarization_chain.predict(text=text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gkePEf4JsrUZ",
        "outputId": "2eff766c-cad0-4291-b330-4625983bf703"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LangChain offers various modules for developing language model applications, which can be used alone for simple applications or combined for more complex ones.'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summarized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSNszdpHsyQp"
      },
      "source": [
        "#====="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vxwbIbhswrL"
      },
      "outputs": [],
      "source": [
        "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
        "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
        "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onKET3HHtzTY"
      },
      "outputs": [],
      "source": [
        "source_language = \"English\"\n",
        "target_language = \"French\"\n",
        "text = \"Your text here\"\n",
        "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HxgS3uset0v4",
        "outputId": "1e9726d2-f110-44e6-f989-ec76eda5a7ef"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Votre texte ici'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translated_text"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
